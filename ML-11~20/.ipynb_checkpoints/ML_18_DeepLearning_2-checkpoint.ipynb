{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c6f2be-e4f4-49e8-b443-7943bf9123ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "# 多层感知机从0实现\n",
    "# 导入所需包\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48d622-5868-4701-ad4b-1b383190f218",
   "metadata": {},
   "source": [
    "### 1. 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5a4e7f-2be5-4543-810d-0a6a8e95e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST数据集，使用多层感知机对图像进行分类\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ff6307-96de-4785-97c0-d1c52ce2e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 8us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9faeab23-0bff-4460-a6f6-eac4b89d11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecd3ce-5c97-433c-8b7c-7fdb60341b48",
   "metadata": {},
   "source": [
    "### 2. 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9991e5e-a489-452a-bf2c-28adc064b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "W1 = tf.Variable(tf.random.normal(shape=(num_inputs, num_hiddens),mean=0, stddev=0.01, dtype=tf.float32))\n",
    "b1 = tf.Variable(tf.zeros(num_hiddens, dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.random.normal(shape=(num_hiddens, num_outputs),mean=0, stddev=0.01, dtype=tf.float32))\n",
    "b2 = tf.Variable(tf.random.normal([num_outputs], stddev=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1ee0d-708d-4fa5-879b-1dea50c46e71",
   "metadata": {},
   "source": [
    "### 3. 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365314f0-4eb0-4d43-afe5-8d7006d03ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用基础的max函数来实现ReLU\n",
    "def relu(x):\n",
    "    return tf.math.maximum(x,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdfcd2c-19ad-4790-bf9c-cefcfaa322ca",
   "metadata": {},
   "source": [
    "### 4. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66072503-865d-4c42-bc65-9dc81ae06a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过reshape函数将每张原始图像改成长度为num_inputs的向量\n",
    "def net(X):\n",
    "    X = tf.reshape(X, shape=[-1, num_inputs])\n",
    "    h = relu(tf.matmul(X, W1) + b1)\n",
    "    return tf.math.softmax(tf.matmul(h, W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd90326-3e68-4314-bdfb-57d0fdc319ac",
   "metadata": {},
   "source": [
    "### 5. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65ef1225-e08b-4f0b-bcae-7e1b8d34db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用Tensorflow提供的包括softmax运算和交叉熵损失计算的函数\n",
    "def loss(y_hat,y_true):\n",
    "    return tf.losses.sparse_categorical_crossentropy(y_true,y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf68f30-3a5c-49ce-b52c-329883a73de5",
   "metadata": {},
   "source": [
    "### 6. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4058d61b-ee7f-40b7-a54b-d4ba42fcbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 描述,对于tensorflow2中，比较的双方必须类型都是int型，所以要将输出和标签都转为int型\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for _, (X, y) in enumerate(data_iter):\n",
    "        y = tf.cast(y,dtype=tf.int64)\n",
    "        acc_sum += np.sum(tf.cast(tf.argmax(net(X), axis=1), dtype=tf.int64) == y)\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b047da54-0ce2-4100-9fe6-f3a73a6bd1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.3019, train acc 0.106, test acc 0.100\n",
      "epoch 2, loss 2.3000, train acc 0.114, test acc 0.100\n",
      "epoch 3, loss 2.2950, train acc 0.139, test acc 0.177\n",
      "epoch 4, loss 2.2817, train acc 0.175, test acc 0.184\n",
      "epoch 5, loss 2.2485, train acc 0.215, test acc 0.195\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "params = [W1, b1, W2, b2]\n",
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_hat = net(X)\n",
    "                l = tf.reduce_sum(loss(y_hat, y))\n",
    "            grads = tape.gradient(l, params)\n",
    "            if trainer is None:\n",
    "                # 如果没有传入优化器，则使用原先编写的小批量随机梯度下降\n",
    "                for i, param in enumerate(params):\n",
    "                    param.assign_sub(lr * grads[i] / batch_size)\n",
    "            else:\n",
    "                # tf.keras.optimizers.SGD 直接使用是随机梯度下降 theta(t+1) = theta(t) - learning_rate * gradient\n",
    "                # 这里使用批量梯度下降，需要对梯度除以 batch_size, 对应原书代码的 trainer.step(batch_size)\n",
    "                trainer.apply_gradients(zip([grad / batch_size for grad in grads], params))  \n",
    "\n",
    "            y = tf.cast(y, dtype=tf.float32)\n",
    "            train_l_sum += l.numpy()\n",
    "            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=1) == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'% (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
