{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64e15c3-a38f-48b7-87f7-1beacd1f8f36",
   "metadata": {},
   "source": [
    "### 文本数据建模流程范例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c164b6c-abbe-4bd6-9fff-16604a6f820a",
   "metadata": {},
   "source": [
    "1. 准备数据\n",
    "- `imdb`数据集的目标是根据电影评论的文本内容预测评论的情感标签\n",
    "- `tensorflow`中完成文本数据预处理的常用方案有两种：\n",
    "  1. 利用`tf.keras.preprocessing`中的`Tokenizer`词典构建工具和`tf.keras.utils.Sequence`构建文本数据生成器管道（较为复杂）\n",
    "  2. 使用`tf.data.Dataset`搭配`.keras.layers.experimental.preprocessing.TextVectorization`预处理层（TensorFlow原生方式，相对简单）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d92001e-f9b0-48e8-82cb-dc5bfe2b7374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 10:43:02.070328: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-18 10:43:02.135062: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-18 10:43:02.136348: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 10:43:03.179329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re,string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a98012-7fef-4546-b91b-41e87851f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../data/imdb/train.csv\"\n",
    "test_data_path =  \"../data/imdb/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58519870-9b5b-4d45-a0d4-9a32f8c67fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c0d63b-234a-4965-9219-513b81eb2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用第二种方法构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line,\"\\t\")\n",
    "    # 在指定的位置插入一个新的维度\n",
    "    # tf.expand_dims 经常用于在输入数据张量上插入批次维度，以便进行批量预测或训练。\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\n",
    "    text = tf.expand_dims(arr[1],axis = 0)\n",
    "    return (text,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6093c0d0-5dd5-40d5-8e72-812b5c95158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 10:47:58.254353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-18 10:47:58.297488: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376b5db0-d621-4276-82b0-1a48b0913832",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13ab406-4951-4c82-ab9b-581c461cea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5182a058-40aa-49a0-9e99-de6a8c43af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time', 'only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me', 'than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also', 'other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, #有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text,label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd3ecb8-73b6-4610-a70b-50055366e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#单词编码\n",
    "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a6e49-5771-4552-b3d3-2009ade60e8b",
   "metadata": {},
   "source": [
    "2. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36fc51e-89f4-4a0c-a26f-6157699aa8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用继承Model基类构建自定义模型\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81bd8082-421a-4b66-8872-3b9b5d267ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 7)            70000     \n",
      "                                                                 \n",
      " conv_1 (Conv1D)             (None, 196, 16)           576       \n",
      "                                                                 \n",
      " pool_1 (MaxPooling1D)       (None, 98, 16)            0         \n",
      "                                                                 \n",
      " conv_2 (Conv1D)             (None, 97, 128)           4224      \n",
      "                                                                 \n",
      " pool_2 (MaxPooling1D)       (None, 48, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6144)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80945 (316.19 KB)\n",
      "Trainable params: 80945 (316.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CnnModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\n",
    "        self.conv_1 = layers.Conv1D(16, kernel_size= 5,name = \"conv_1\",activation = \"relu\")\n",
    "        self.pool_1 = layers.MaxPool1D(name = \"pool_1\")\n",
    "        self.conv_2 = layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\")\n",
    "        self.pool_2 = layers.MaxPool1D(name = \"pool_2\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(CnnModel,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)\n",
    "\n",
    "    # 用于显示Output Shape\n",
    "    def summary(self):\n",
    "        x_input = layers.Input(shape = MAX_LEN)\n",
    "        output = self.call(x_input)\n",
    "        model = tf.keras.Model(inputs = x_input,outputs = output)\n",
    "        model.summary()\n",
    "\n",
    "model = CnnModel()\n",
    "model.build(input_shape =(None,MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454aff7d-3049-446d-82b3-98b658983a2a",
   "metadata": {},
   "source": [
    "3. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab28e653-d78b-40bb-8497-d861445c1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义训练循环训练模型\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    today_ts = tf.timestamp()%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "\n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "\n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8+timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42e1546a-b54b-4702-960d-ade9fe72b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5407f36-ccd0-458f-8521-e0b70111236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features,training = True)\n",
    "        loss = loss_func(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, features, labels):\n",
    "    predictions = model(features,training = False)\n",
    "    batch_loss = loss_func(labels, predictions)\n",
    "    valid_loss.update_state(batch_loss)\n",
    "    valid_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "def train_model(model,ds_train,ds_valid,epochs):\n",
    "    for epoch in tf.range(1,epochs+1):\n",
    "\n",
    "        for features, labels in ds_train:\n",
    "            train_step(model,features,labels)\n",
    "\n",
    "        for features, labels in ds_valid:\n",
    "            valid_step(model,features,labels)\n",
    "\n",
    "        #此处logs模板需要根据metric具体情况修改\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}' \n",
    "\n",
    "        if epoch%1==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c4927ca-89b5-44a0-9954-1d86d6ec3e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================10:50:56\n",
      "Epoch=1,Loss:0.433327377,Accuracy:0.7744,Valid Loss:0.317821354,Valid Accuracy:0.863\n",
      "\n",
      "================================================================================10:51:03\n",
      "Epoch=2,Loss:0.221809298,Accuracy:0.9127,Valid Loss:0.346467316,Valid Accuracy:0.8644\n",
      "\n",
      "================================================================================10:51:11\n",
      "Epoch=3,Loss:0.143342242,Accuracy:0.94595,Valid Loss:0.421649843,Valid Accuracy:0.8606\n",
      "\n",
      "================================================================================10:51:18\n",
      "Epoch=4,Loss:0.0819885433,Accuracy:0.9717,Valid Loss:0.59040308,Valid Accuracy:0.8522\n",
      "\n",
      "================================================================================10:51:25\n",
      "Epoch=5,Loss:0.0368908308,Accuracy:0.9878,Valid Loss:0.829206944,Valid Accuracy:0.8526\n",
      "\n",
      "================================================================================10:51:32\n",
      "Epoch=6,Loss:0.022493124,Accuracy:0.9925,Valid Loss:1.05547059,Valid Accuracy:0.845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model,ds_train,ds_test,epochs = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584de96a-21f2-45eb-ab35-555d509d10fb",
   "metadata": {},
   "source": [
    "4. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94415b4-6102-48d6-ae48-09ee6ee4498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过自定义训练循环训练的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法\n",
    "def evaluate_model(model,ds_valid):\n",
    "    for features, labels in ds_valid:\n",
    "         valid_step(model,features,labels)\n",
    "    logs = 'Valid Loss:{},Valid Accuracy:{}' \n",
    "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\n",
    "\n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2160ab0a-cfe4-4b84-aba8-6f979b439b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss:1.05547059,Valid Accuracy:0.845\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568b1d8-7e1a-4644-b387-9f7295aa22cc",
   "metadata": {},
   "source": [
    "5. 使用模型预测st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f677fc4d-69d5-433f-bd68-193f5bbc8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9066615 ],\n",
       "       [0.9999972 ],\n",
       "       [0.9999982 ],\n",
       "       ...,\n",
       "       [0.9999804 ],\n",
       "       [0.22427195],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用model.predict(ds_test)方法\n",
    "model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae6f491-46c4-420d-875c-c8e758bc7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.0666139e-01]\n",
      " [9.9999720e-01]\n",
      " [9.9999821e-01]\n",
      " [4.3126863e-07]\n",
      " [9.5012975e-01]\n",
      " [5.7610436e-05]\n",
      " [7.9664034e-08]\n",
      " [2.2415623e-01]\n",
      " [9.9990994e-01]\n",
      " [9.9996918e-01]\n",
      " [1.0000000e+00]\n",
      " [7.4551636e-01]\n",
      " [1.4705578e-10]\n",
      " [9.9930972e-01]\n",
      " [2.4761547e-07]\n",
      " [9.9256420e-01]\n",
      " [3.5520742e-04]\n",
      " [1.1825867e-01]\n",
      " [1.6452361e-05]\n",
      " [9.9550885e-01]], shape=(20, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x_test,_ in ds_test.take(1):\n",
    "    print(model(x_test))\n",
    "    #以下方法等价：\n",
    "    #print(model.call(x_test))\n",
    "    #print(model.predict_on_batch(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2bd8b-8315-4735-8760-03ed596f2047",
   "metadata": {},
   "source": [
    "6. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90b07c64-a3a2-471f-a090-7fde51137bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/imdb/tf_model_savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/imdb/tf_model_savedmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export saved model.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9066615 ],\n",
       "       [0.9999972 ],\n",
       "       [0.9999982 ],\n",
       "       ...,\n",
       "       [0.9999804 ],\n",
       "       [0.22427195],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('../data/imdb/tf_model_savedmodel', save_format=\"tf\")\n",
    "print('export saved model.')\n",
    "\n",
    "model_loaded = tf.keras.models.load_model('../data/imdb/tf_model_savedmodel')\n",
    "model_loaded.predict(ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
